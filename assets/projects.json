{
"largeweak": {
"id" : "largeweak",
"imgz": ["images/weakfeat.jpg"],
"subtitle" : "Armand Joulin*, Laurens van der Maaten*, Allan Jabri, Nicolas Vasilache (* equal contribution)",
"title": "Learning Visual Features from Large Weakly Supervised Data",
"desc": "https://arxiv.org/abs/1511.02251"},

"vqa": {
"id" : "vqa",
"imgz": ["images/revisitingvqa.jpg"],
"subtitle" : "Allan Jabri, Armand Joulin, Laurens van der Maaten",
"title": "Revisiting Visual Question Answering Baselines",
"desc": "http://arxiv.org/abs/1606.08390"},

"visualngrams": {
"id" : "visualngrams",
"imgz": ["images/visualngrams.jpg"],
"subtitle" : "Ang Li, Allan Jabri, Armand Joulin, Laurens van der Maaten",
"title": "Learning Visual N-grams from Web Data",
"desc": "https://arxiv.org/abs/1612.09161"},

"mainnips": {
"id" : "mainnips",
"imgz": ["images/nips2016.jpg"],
"subtitle" : "Co-Organizer",
"title": "Machine Intelligence Workshop at NIPS 2016",
"desc": "https://mainatnips.github.io"},

"slang": {
"id" : "slang",
"imgz": ["images/ring.svg"],
"subtitle" : "Translating French SMS Slang",
"title": "Translating French SMS Slang",
"desc": "Translation is most interesting when it helps people communicate, not translate dish washer manuals (which represents the majority of training data for most translators, like Google's). Here, we trained a SMT model for French SMS Slang <-> French <-> English. Also created a demo for generating French SMS from English or French. Used Moses SMT with dataset from (Fairon and Paumier, 2006)."},

"recon": {
"id" : "recon",
"imgz": ["http://sunglass.cs.princeton.edu/figures/spel_slant.png", "http://sunglass.cs.princeton.edu/figures/spel1.png","http://sunglass.cs.princeton.edu/figures/spel_living.png", "http://sunglass.cs.princeton.edu/figures/spel_kitchen.jpg", "http://sunglass.cs.princeton.edu/figures/spel_br.png","http://sunglass.cs.princeton.edu/figures/outdoor.png"],
"subtitle" : "Discovering the 3D structure of Habitual Scenes",
"title": "3D Reconstruction from Routine Motion",
"desc": "Everyday, we visit the same spaces in similar ways, though our behavior is never identical. Quality Structure from Motion reconstructions require many images from different perspectives as well as loop-closure. The key observation - that we naturally experience the same scenes from multiple views in our everyday lives - inspires us to treat human behavior as a big data solution for SFM. With GPS and first-person RGB lifelog data, we reconstruct the scenes of our everyday lives in 3D. With the resulting camera poses, we also might gather a sense of activity hotspots and patterns. <br> <br> (Part of Undergraduate Thesis)"},

"topics": {
"id" : "topics",
"imgz": ["http://sunglass.cs.princeton.edu/figures/topics_1.png","http://sunglass.cs.princeton.edu/figures/topics_2.png", "http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.04.59%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.05.20%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.05.40%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.06.06%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.06.39%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.07.02%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.07.25%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.07.39%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.07.54%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.08.07%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%207.56.30%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%207.59.33%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.00.09%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.00.23%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.00.32%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.01.03%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.01.14%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.01.23%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.01.30%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.01.41%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.01.55%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.17.21%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.17.52%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.18.11%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.18.24%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.18.52%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.19.04%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.19.14%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.19.44%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.20.00%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.20.14%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.26.16%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.26.33%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.26.55%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.47.36%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.47.51%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.48.04%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.48.19%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.48.34%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.48.48%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.49.00%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.49.12%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.49.21%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.49.35%20PM.png"],
"subtitle" : "Towards First-Person Visual Context-Awareness",
"title" : "Discovering Visual Scene Topics of Routine Spaces",
"desc" : "For a smart assistant, understanding a user's visual context is not as simple as atomically classifying scenes. The human experience of being in a place is noisy; we constantly engage with different parts of a scene, gazing here, focusing there. As humans, we have learned to understand a place as something more stable than the type of scene we see in the moment. Using CNN Scene Classification with LDA topic modeling, we discover the reoccurring visual scenes, or rather, the places of our everyday lives from RGB lifelog data. A space is an objective, physical manifestation. A place is a personal narration of a space. This narration is habitual, repetitive. In this project, we aim to discover places intrinsically repeated in habit, formulating them as visual scene topics. <br><br>(Part of Undergraduate Thesis)"},

"realtime": {
"id" : "realtime",
"imgz": ["images/ring.svg"],
"title" : "Real-Time Scene Classification on Google Glass",
"subtitle" : "CNN Scene Classification in First-Person in Real-Time",
"desc" : "This project bridges a Google Glass with the processing capability of CUDA-enabled Caffe. Frames are streamed over TCP at ~ 5 fps and classified in real-time server-side. The top scene labels are displayed client-side on the Glass. The CNN is based on PlaceNet (Bo et al, 2014). Demo available here:"},

"actions": {
"id" : "actions",
"imgz": ["images/ring.svg"],
"title" : "Place Aware Actions on Google Glass",
"subtitle" : "A Very Simple Visually Aware Personal Assistant",
"desc" : "Inspired by the desire to build a very simple personal assistant, this project allows users to tag landmarks with simple actions (using a web GUI). Scene description was implemented as a combination of GIST with semantic hashing for efficient retrieval. Implemented on the Android Platform, for Google Glass."},

"timelapse": {
"id" : "timelapse",
"imgz": ["http://visiongpu.cs.princeton.edu/SUNglass/cgi-bin/tmp/planar_medians/799/animate.gif"],
"title" : "Mining Time-lapses from Google Glass Lifelog Data",
"subtitle" : "Accidental Time-lapses of Routine Spaces",
"desc" : "In the future, first-person cameras will be ubiquitous. Buried in this data are views of the same areas over time. Using SFM, MVS, and other computational photography techniques, we craft time-lapses for chosen reference views. A GUI is provided for selecting the criteria for images included in the time-lapse mining."},

"rgbd": {
"id" : "rgbd",
"imgz": ["http://sunglass.cs.princeton.edu/figures/gears.png"],
"subtitle" : "Augmented Visualizations of Routine Spaces and Objects",
"title" : "First-Person RGBD",
"desc" : "Work In Progress. Using Structure IO sensor to lifelog routine behavior."},

"flare": {
"id" : "flare",
"imgz": ["http://sunglass.cs.princeton.edu/figures/flare_icon.png","http://sunglass.cs.princeton.edu/figures/flare_lifecycle.jpg","http://sunglass.cs.princeton.edu/figures/flare_lifecycle.jpg"],
"title" : "Flare",
"desc" : "An application that allows users to control how their location data is stored, while empowering them with the ability to broadcast their location to those who care the most.",
"subtitle" : "Ephemeral Location Sharing"}
}