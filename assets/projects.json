{

"timecycle": {
    "id" : "timecycle",
    "imgz": ["images/timecycle_fig1.jpg"]
},
"carml": {
    "id" : "carml",
    "imgz": ["images/carml_fig1.png"]
},
"videowalk": {
    "id" : "videowalk",
    "imgz": ["images/teaser_animation.gif"]
},      
"largeweak": {
"id" : "largeweak",
"imgz": ["images/weakfeat.jpg"],
"subtitle" : "Armand Joulin*, Laurens van der Maaten*, Allan Jabri, Nicolas Vasilache (* equal contribution)",
"title": "Learning Visual Features from Large Weakly Supervised Data",
"desc": "https://arxiv.org/abs/1511.02251"},

"vqa": {
"id" : "vqa",
"imgz": ["images/revisitingvqa.jpg"],
"subtitle" : "Allan Jabri, Armand Joulin, Laurens van der Maaten",
"title": "Revisiting Visual Question Answering Baselines",
"desc": "http://arxiv.org/abs/1606.08390"},

"visualngrams": {
"id" : "visualngrams",
"imgz": ["images/visualngrams.jpg"],
"subtitle" : "Ang Li, Allan Jabri, Armand Joulin, Laurens van der Maaten",
"title": "Learning Visual N-grams from Web Data",
"desc": "https://arxiv.org/abs/1612.09161"},

"commai-iclr": {
"id" : "commai-iclr",
"imgz": ["images/ring.svg"],
"subtitle" : "Marco Baroni, Armand Joulin, Allan Jabri, Germàn Kruszewski, Angeliki Lazaridou, Klemen Simonic, Tomas Mikolov",
"title": "CommAI: Evaluating the first steps towards a useful general AI",
"desc": "https://arxiv.org/abs/1701.08954"},

"commai-env": {
"id" : "commai-env",
"imgz": ["images/ring.svg"],
"subtitle" : "Marco Baroni, Armand Joulin, Allan Jabri, Germàn Kruszewski, Angeliki Lazaridou, Klemen Simonic, Tomas Mikolov",
"title": "CommAI-env",
"desc": "https://github.com/facebookresearch/CommAI-env"},

"mainnips": {
"id" : "mainnips",
"imgz": ["images/nips2016.jpg"],
"subtitle" : "Co-Organizer",
"title": "Machine Intelligence Workshop at NIPS 2016",
"desc": "https://mainatnips.github.io"},

"slang": {
"id" : "slang",
"imgz": ["images/ring.svg"],
"subtitle" : "Translating French SMS Slang",
"title": "Translating French SMS Slang",
"desc": "Translation is most interesting when it helps people communicate, not translate dish washer manuals (which represents the majority of training data for most translators, like Google's). Here, we trained a SMT model for French SMS Slang <-> French <-> English. Also created a demo for generating French SMS from English or French. Used Moses SMT with dataset from (Fairon and Paumier, 2006)."},

"recon": {
"id" : "recon",
"imgz": ["http://sunglass.cs.princeton.edu/figures/spel_slant.png", "http://sunglass.cs.princeton.edu/figures/spel1.png","http://sunglass.cs.princeton.edu/figures/spel_living.png", "http://sunglass.cs.princeton.edu/figures/spel_kitchen.jpg", "http://sunglass.cs.princeton.edu/figures/spel_br.png","http://sunglass.cs.princeton.edu/figures/outdoor.png"],
"subtitle" : "Discovering the 3D structure of Habitual Scenes",
"title": "3D Reconstruction from Routine Motion",
"desc": "Structure from motion requires many images from different perspectives as well as loop-closure. The key observation - that we naturally experience the same scenes from multiple views in our everyday lives - inspires us to treat human behavior as a data solution for SFM. With GPS metadata and first-person RGB lifelog data, we efficiently reconstruct the scenes of everyday life in 3D. This spatial information allows us to ground learned semantic scene representations in 3D space. <br> <br> (Part of Undergraduate Thesis)"},

"topics": {
"id" : "topics",
"imgz": ["http://sunglass.cs.princeton.edu/figures/topics_1.png","http://sunglass.cs.princeton.edu/figures/topics_2.png", "http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.04.59%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.05.20%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.05.40%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.06.06%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.06.39%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.07.02%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.07.25%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.07.39%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.07.54%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%206.08.07%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%207.56.30%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%207.59.33%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.00.09%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.00.23%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.00.32%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.01.03%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.01.14%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.01.23%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.01.30%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.01.41%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.01.55%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.17.21%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.17.52%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.18.11%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.18.24%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.18.52%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.19.04%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.19.14%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.19.44%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.20.00%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.20.14%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.26.16%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.26.33%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%208.26.55%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.47.36%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.47.51%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.48.04%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.48.19%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.48.34%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.48.48%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.49.00%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.49.12%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.49.21%20PM.png","http://sunglass.cs.princeton.edu/pics/Screen%20Shot%202015-04-29%20at%209.49.35%20PM.png"],
"subtitle" : "Towards First-Person Visual Context-Awareness",
"title" : "Discovering Visual Scene Topics of Routine Spaces",
"desc" : "Understanding a egocentric visual context is not as simple as independently classifying scenes. The human experience of being in a place is noisy: we constantly engage with different parts of a scene, gazing here, focusing there. As humans, we have learned to understand a place as something more stable than the type of scene we see in the moment. In this work, we model temporal scene semantics of routine RGB egocentric data with deep visual representations and hierarchical generative models. This representation of visual context is spatially grounded with structure from motion. <br> A space is an objective, physical manifestation. A place is a personal semantic narration of a space. In this project, we aim to discover and model this habitual narration. <br><br>(Part of Undergraduate Thesis)"},

"realtime": {
"id" : "realtime",
"imgz": ["images/ring.svg"],
"title" : "Real-Time Scene Recognition on Google Glass",
"subtitle" : "CNN Scene Recognition in First-Person in Real-Time",
"desc" : "This project bridges a Google Glass with the processing capability of CUDA-enabled Caffe. Frames are streamed over TCP and classified in real-time server-side. The top scene labels are displayed client-side on the Glass. The CNN is trained on Places (Zhou et al, 2014)."},

"timelapse": {
"id" : "timelapse",
"imgz": ["http://visiongpu.cs.princeton.edu/SUNglass/cgi-bin/tmp/planar_medians/799/animate.gif"],
"title" : "Mining Time-lapses from Google Glass Lifelog Data",
"subtitle" : "Accidental Time-lapses of Routine Spaces",
"desc" : "In the future, first-person cameras will be ubiquitous. Buried in this data are views of the same areas over time. Using SFM, MVS, and other computational photography techniques, we craft time-lapses for chosen reference views. A GUI is provided for selecting the criteria for images included in the time-lapse mining."},

"rgbd": {
"id" : "rgbd",
"imgz": ["http://sunglass.cs.princeton.edu/figures/gears.png"],
"subtitle" : "Augmented Visualizations of Routine Spaces and Objects",
"title" : "First-Person RGBD",
"desc" : "Work In Progress (last update 11/2015). Using Structure IO sensor to understand routine behavior."},

"flare": {
"id" : "flare",
"imgz": ["http://sunglass.cs.princeton.edu/figures/flare_icon.png","http://sunglass.cs.princeton.edu/figures/flare_lifecycle.jpg","http://sunglass.cs.princeton.edu/figures/flare_lifecycle.jpg"],
"title" : "Flare",
"desc" : "An application that allows users to control how their location data is stored, while empowering them with the ability to broadcast their location to those who care the most.",
"subtitle" : "Ephemeral Location Sharing"}
}